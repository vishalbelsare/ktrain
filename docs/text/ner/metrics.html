<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.10.0" />
<title>ktrain.text.ner.metrics API documentation</title>
<meta name="description" content="Metrics to assess performance on sequence labeling task given prediction
Functions named as ``*_score`` return a scalar value to maximize: the higher
â€¦" />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>ktrain.text.ner.metrics</code></h1>
</header>
<section id="section-intro">
<p>Metrics to assess performance on sequence labeling task given prediction
Functions named as <code>*_score</code> return a scalar value to maximize: the higher
the better
Reference: seqeval==0.0.19</p>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">&#34;&#34;&#34;Metrics to assess performance on sequence labeling task given prediction
Functions named as ``*_score`` return a scalar value to maximize: the higher
the better
Reference: seqeval==0.0.19
&#34;&#34;&#34;

from __future__ import absolute_import, division, print_function

import warnings
from collections import defaultdict

import numpy as np


def get_entities(seq, suffix=False):
    &#34;&#34;&#34;Gets entities from sequence.

    Args:
        seq (list): sequence of labels.

    Returns:
        list: list of (chunk_type, chunk_start, chunk_end).

    Example:
        &gt;&gt;&gt; from seqeval.metrics.sequence_labeling import get_entities
        &gt;&gt;&gt; seq = [&#39;B-PER&#39;, &#39;I-PER&#39;, &#39;O&#39;, &#39;B-LOC&#39;]
        &gt;&gt;&gt; get_entities(seq)
        [(&#39;PER&#39;, 0, 1), (&#39;LOC&#39;, 3, 3)]
    &#34;&#34;&#34;

    def _validate_chunk(chunk, suffix):
        if chunk in [&#34;O&#34;, &#34;B&#34;, &#34;I&#34;, &#34;E&#34;, &#34;S&#34;]:
            return

        if suffix:
            if not (
                chunk.endswith(&#34;-B&#34;)
                or chunk.endswith(&#34;-I&#34;)
                or chunk.endswith(&#34;-E&#34;)
                or chunk.endswith(&#34;-S&#34;)
            ):
                warnings.warn(&#34;{} seems not to be NE tag.&#34;.format(chunk))

        else:
            if not (
                chunk.startswith(&#34;B-&#34;)
                or chunk.startswith(&#34;I-&#34;)
                or chunk.startswith(&#34;E-&#34;)
                or chunk.startswith(&#34;S-&#34;)
            ):
                warnings.warn(&#34;{} seems not to be NE tag.&#34;.format(chunk))

    # for nested list
    if any(isinstance(s, list) for s in seq):
        seq = [item for sublist in seq for item in sublist + [&#34;O&#34;]]

    prev_tag = &#34;O&#34;
    prev_type = &#34;&#34;
    begin_offset = 0
    chunks = []
    for i, chunk in enumerate(seq + [&#34;O&#34;]):
        _validate_chunk(chunk, suffix)

        if suffix:
            tag = chunk[-1]
            type_ = chunk[:-1].rsplit(&#34;-&#34;, maxsplit=1)[0] or &#34;_&#34;
        else:
            tag = chunk[0]
            type_ = chunk[1:].split(&#34;-&#34;, maxsplit=1)[-1] or &#34;_&#34;

        if end_of_chunk(prev_tag, tag, prev_type, type_):
            chunks.append((prev_type, begin_offset, i - 1))
        if start_of_chunk(prev_tag, tag, prev_type, type_):
            begin_offset = i
        prev_tag = tag
        prev_type = type_

    return chunks


def end_of_chunk(prev_tag, tag, prev_type, type_):
    &#34;&#34;&#34;Checks if a chunk ended between the previous and current word.

    Args:
        prev_tag: previous chunk tag.
        tag: current chunk tag.
        prev_type: previous type.
        type_: current type.

    Returns:
        chunk_end: boolean.
    &#34;&#34;&#34;
    chunk_end = False

    if prev_tag == &#34;E&#34;:
        chunk_end = True
    if prev_tag == &#34;S&#34;:
        chunk_end = True

    if prev_tag == &#34;B&#34; and tag == &#34;B&#34;:
        chunk_end = True
    if prev_tag == &#34;B&#34; and tag == &#34;S&#34;:
        chunk_end = True
    if prev_tag == &#34;B&#34; and tag == &#34;O&#34;:
        chunk_end = True
    if prev_tag == &#34;I&#34; and tag == &#34;B&#34;:
        chunk_end = True
    if prev_tag == &#34;I&#34; and tag == &#34;S&#34;:
        chunk_end = True
    if prev_tag == &#34;I&#34; and tag == &#34;O&#34;:
        chunk_end = True

    if prev_tag != &#34;O&#34; and prev_tag != &#34;.&#34; and prev_type != type_:
        chunk_end = True

    return chunk_end


def start_of_chunk(prev_tag, tag, prev_type, type_):
    &#34;&#34;&#34;Checks if a chunk started between the previous and current word.

    Args:
        prev_tag: previous chunk tag.
        tag: current chunk tag.
        prev_type: previous type.
        type_: current type.

    Returns:
        chunk_start: boolean.
    &#34;&#34;&#34;
    chunk_start = False

    if tag == &#34;B&#34;:
        chunk_start = True
    if tag == &#34;S&#34;:
        chunk_start = True

    if prev_tag == &#34;E&#34; and tag == &#34;E&#34;:
        chunk_start = True
    if prev_tag == &#34;E&#34; and tag == &#34;I&#34;:
        chunk_start = True
    if prev_tag == &#34;S&#34; and tag == &#34;E&#34;:
        chunk_start = True
    if prev_tag == &#34;S&#34; and tag == &#34;I&#34;:
        chunk_start = True
    if prev_tag == &#34;O&#34; and tag == &#34;E&#34;:
        chunk_start = True
    if prev_tag == &#34;O&#34; and tag == &#34;I&#34;:
        chunk_start = True

    if tag != &#34;O&#34; and tag != &#34;.&#34; and prev_type != type_:
        chunk_start = True

    return chunk_start


def f1_score(y_true, y_pred, average=&#34;micro&#34;, suffix=False):
    &#34;&#34;&#34;Compute the F1 score.

    The F1 score can be interpreted as a weighted average of the precision and
    recall, where an F1 score reaches its best value at 1 and worst score at 0.
    The relative contribution of precision and recall to the F1 score are
    equal. The formula for the F1 score is::

        F1 = 2 * (precision * recall) / (precision + recall)

    Args:
        y_true : 2d array. Ground truth (correct) target values.
        y_pred : 2d array. Estimated targets as returned by a tagger.

    Returns:
        score : float.

    Example:
        &gt;&gt;&gt; from seqeval.metrics import f1_score
        &gt;&gt;&gt; y_true = [[&#39;O&#39;, &#39;O&#39;, &#39;O&#39;, &#39;B-MISC&#39;, &#39;I-MISC&#39;, &#39;I-MISC&#39;, &#39;O&#39;], [&#39;B-PER&#39;, &#39;I-PER&#39;, &#39;O&#39;]]
        &gt;&gt;&gt; y_pred = [[&#39;O&#39;, &#39;O&#39;, &#39;B-MISC&#39;, &#39;I-MISC&#39;, &#39;I-MISC&#39;, &#39;I-MISC&#39;, &#39;O&#39;], [&#39;B-PER&#39;, &#39;I-PER&#39;, &#39;O&#39;]]
        &gt;&gt;&gt; f1_score(y_true, y_pred)
        0.50
    &#34;&#34;&#34;
    true_entities = set(get_entities(y_true, suffix))
    pred_entities = set(get_entities(y_pred, suffix))

    nb_correct = len(true_entities &amp; pred_entities)
    nb_pred = len(pred_entities)
    nb_true = len(true_entities)

    p = nb_correct / nb_pred if nb_pred &gt; 0 else 0
    r = nb_correct / nb_true if nb_true &gt; 0 else 0
    score = 2 * p * r / (p + r) if p + r &gt; 0 else 0

    return score


def accuracy_score(y_true, y_pred):
    &#34;&#34;&#34;Accuracy classification score.

    In multilabel classification, this function computes subset accuracy:
    the set of labels predicted for a sample must *exactly* match the
    corresponding set of labels in y_true.

    Args:
        y_true : 2d array. Ground truth (correct) target values.
        y_pred : 2d array. Estimated targets as returned by a tagger.

    Returns:
        score : float.

    Example:
        &gt;&gt;&gt; from seqeval.metrics import accuracy_score
        &gt;&gt;&gt; y_true = [[&#39;O&#39;, &#39;O&#39;, &#39;O&#39;, &#39;B-MISC&#39;, &#39;I-MISC&#39;, &#39;I-MISC&#39;, &#39;O&#39;], [&#39;B-PER&#39;, &#39;I-PER&#39;, &#39;O&#39;]]
        &gt;&gt;&gt; y_pred = [[&#39;O&#39;, &#39;O&#39;, &#39;B-MISC&#39;, &#39;I-MISC&#39;, &#39;I-MISC&#39;, &#39;I-MISC&#39;, &#39;O&#39;], [&#39;B-PER&#39;, &#39;I-PER&#39;, &#39;O&#39;]]
        &gt;&gt;&gt; accuracy_score(y_true, y_pred)
        0.80
    &#34;&#34;&#34;
    if any(isinstance(s, list) for s in y_true):
        y_true = [item for sublist in y_true for item in sublist]
        y_pred = [item for sublist in y_pred for item in sublist]

    nb_correct = sum(y_t == y_p for y_t, y_p in zip(y_true, y_pred))
    nb_true = len(y_true)

    score = nb_correct / nb_true

    return score


def precision_score(y_true, y_pred, average=&#34;micro&#34;, suffix=False):
    &#34;&#34;&#34;Compute the precision.

    The precision is the ratio ``tp / (tp + fp)`` where ``tp`` is the number of
    true positives and ``fp`` the number of false positives. The precision is
    intuitively the ability of the classifier not to label as positive a sample.

    The best value is 1 and the worst value is 0.

    Args:
        y_true : 2d array. Ground truth (correct) target values.
        y_pred : 2d array. Estimated targets as returned by a tagger.

    Returns:
        score : float.

    Example:
        &gt;&gt;&gt; from seqeval.metrics import precision_score
        &gt;&gt;&gt; y_true = [[&#39;O&#39;, &#39;O&#39;, &#39;O&#39;, &#39;B-MISC&#39;, &#39;I-MISC&#39;, &#39;I-MISC&#39;, &#39;O&#39;], [&#39;B-PER&#39;, &#39;I-PER&#39;, &#39;O&#39;]]
        &gt;&gt;&gt; y_pred = [[&#39;O&#39;, &#39;O&#39;, &#39;B-MISC&#39;, &#39;I-MISC&#39;, &#39;I-MISC&#39;, &#39;I-MISC&#39;, &#39;O&#39;], [&#39;B-PER&#39;, &#39;I-PER&#39;, &#39;O&#39;]]
        &gt;&gt;&gt; precision_score(y_true, y_pred)
        0.50
    &#34;&#34;&#34;
    true_entities = set(get_entities(y_true, suffix))
    pred_entities = set(get_entities(y_pred, suffix))

    nb_correct = len(true_entities &amp; pred_entities)
    nb_pred = len(pred_entities)

    score = nb_correct / nb_pred if nb_pred &gt; 0 else 0

    return score


def recall_score(y_true, y_pred, average=&#34;micro&#34;, suffix=False):
    &#34;&#34;&#34;Compute the recall.

    The recall is the ratio ``tp / (tp + fn)`` where ``tp`` is the number of
    true positives and ``fn`` the number of false negatives. The recall is
    intuitively the ability of the classifier to find all the positive samples.

    The best value is 1 and the worst value is 0.

    Args:
        y_true : 2d array. Ground truth (correct) target values.
        y_pred : 2d array. Estimated targets as returned by a tagger.

    Returns:
        score : float.

    Example:
        &gt;&gt;&gt; from seqeval.metrics import recall_score
        &gt;&gt;&gt; y_true = [[&#39;O&#39;, &#39;O&#39;, &#39;O&#39;, &#39;B-MISC&#39;, &#39;I-MISC&#39;, &#39;I-MISC&#39;, &#39;O&#39;], [&#39;B-PER&#39;, &#39;I-PER&#39;, &#39;O&#39;]]
        &gt;&gt;&gt; y_pred = [[&#39;O&#39;, &#39;O&#39;, &#39;B-MISC&#39;, &#39;I-MISC&#39;, &#39;I-MISC&#39;, &#39;I-MISC&#39;, &#39;O&#39;], [&#39;B-PER&#39;, &#39;I-PER&#39;, &#39;O&#39;]]
        &gt;&gt;&gt; recall_score(y_true, y_pred)
        0.50
    &#34;&#34;&#34;
    true_entities = set(get_entities(y_true, suffix))
    pred_entities = set(get_entities(y_pred, suffix))

    nb_correct = len(true_entities &amp; pred_entities)
    nb_true = len(true_entities)

    score = nb_correct / nb_true if nb_true &gt; 0 else 0

    return score


def performance_measure(y_true, y_pred):
    &#34;&#34;&#34;
    Compute the performance metrics: TP, FP, FN, TN

    Args:
        y_true : 2d array. Ground truth (correct) target values.
        y_pred : 2d array. Estimated targets as returned by a tagger.

    Returns:
        performance_dict : dict

    Example:
        &gt;&gt;&gt; from seqeval.metrics import performance_measure
        &gt;&gt;&gt; y_true = [[&#39;O&#39;, &#39;O&#39;, &#39;O&#39;, &#39;B-MISC&#39;, &#39;I-MISC&#39;, &#39;O&#39;, &#39;B-ORG&#39;], [&#39;B-PER&#39;, &#39;I-PER&#39;, &#39;O&#39;, &#39;B-PER&#39;]]
        &gt;&gt;&gt; y_pred = [[&#39;O&#39;, &#39;O&#39;, &#39;B-MISC&#39;, &#39;I-MISC&#39;, &#39;I-MISC&#39;, &#39;O&#39;, &#39;O&#39;], [&#39;B-PER&#39;, &#39;I-PER&#39;, &#39;O&#39;, &#39;B-MISC&#39;]]
        &gt;&gt;&gt; performance_measure(y_true, y_pred)
        {&#39;TP&#39;: 3, &#39;FP&#39;: 3, &#39;FN&#39;: 1, &#39;TN&#39;: 4}
    &#34;&#34;&#34;
    performance_dict = dict()
    if any(isinstance(s, list) for s in y_true):
        y_true = [item for sublist in y_true for item in sublist]
        y_pred = [item for sublist in y_pred for item in sublist]
    performance_dict[&#34;TP&#34;] = sum(
        y_t == y_p for y_t, y_p in zip(y_true, y_pred) if ((y_t != &#34;O&#34;) or (y_p != &#34;O&#34;))
    )
    performance_dict[&#34;FP&#34;] = sum(
        ((y_t != y_p) and (y_p != &#34;O&#34;)) for y_t, y_p in zip(y_true, y_pred)
    )
    performance_dict[&#34;FN&#34;] = sum(
        ((y_t != &#34;O&#34;) and (y_p == &#34;O&#34;)) for y_t, y_p in zip(y_true, y_pred)
    )
    performance_dict[&#34;TN&#34;] = sum(
        (y_t == y_p == &#34;O&#34;) for y_t, y_p in zip(y_true, y_pred)
    )

    return performance_dict


def classification_report(y_true, y_pred, digits=2, suffix=False, output_dict=False):
    &#34;&#34;&#34;Build a text report showing the main classification metrics.

    Args:
        y_true : 2d array. Ground truth (correct) target values.
        y_pred : 2d array. Estimated targets as returned by a classifier.
        digits : int. Number of digits for formatting output floating point values.
        output_dict : bool(default=False). If True, return output as dict else str.

    Returns:
        report : string/dict. Summary of the precision, recall, F1 score for each class.

    Examples:
        &gt;&gt;&gt; from seqeval.metrics import classification_report
        &gt;&gt;&gt; y_true = [[&#39;O&#39;, &#39;O&#39;, &#39;O&#39;, &#39;B-MISC&#39;, &#39;I-MISC&#39;, &#39;I-MISC&#39;, &#39;O&#39;], [&#39;B-PER&#39;, &#39;I-PER&#39;, &#39;O&#39;]]
        &gt;&gt;&gt; y_pred = [[&#39;O&#39;, &#39;O&#39;, &#39;B-MISC&#39;, &#39;I-MISC&#39;, &#39;I-MISC&#39;, &#39;I-MISC&#39;, &#39;O&#39;], [&#39;B-PER&#39;, &#39;I-PER&#39;, &#39;O&#39;]]
        &gt;&gt;&gt; print(classification_report(y_true, y_pred))
                     precision    recall  f1-score   support
        &lt;BLANKLINE&gt;
               MISC       0.00      0.00      0.00         1
                PER       1.00      1.00      1.00         1
        &lt;BLANKLINE&gt;
          micro avg       0.50      0.50      0.50         2
          macro avg       0.50      0.50      0.50         2
       weighted avg       0.50      0.50      0.50         2
        &lt;BLANKLINE&gt;
    &#34;&#34;&#34;
    true_entities = set(get_entities(y_true, suffix))
    pred_entities = set(get_entities(y_pred, suffix))

    name_width = 0
    d1 = defaultdict(set)
    d2 = defaultdict(set)
    for e in true_entities:
        d1[e[0]].add((e[1], e[2]))
        name_width = max(name_width, len(e[0]))
    for e in pred_entities:
        d2[e[0]].add((e[1], e[2]))

    avg_types = [&#34;micro avg&#34;, &#34;macro avg&#34;, &#34;weighted avg&#34;]

    if output_dict:
        report_dict = dict()
    else:
        avg_width = max([len(x) for x in avg_types])
        width = max(name_width, avg_width, digits)
        headers = [&#34;precision&#34;, &#34;recall&#34;, &#34;f1-score&#34;, &#34;support&#34;]
        head_fmt = &#34;{:&gt;{width}s} &#34; + &#34; {:&gt;9}&#34; * len(headers)
        report = head_fmt.format(&#34;&#34;, *headers, width=width)
        report += &#34;\n\n&#34;

        row_fmt = &#34;{:&gt;{width}s} &#34; + &#34; {:&gt;9.{digits}f}&#34; * 3 + &#34; {:&gt;9}\n&#34;

    ps, rs, f1s, s = [], [], [], []
    for type_name in sorted(d1.keys()):
        true_entities = d1[type_name]
        pred_entities = d2[type_name]
        nb_correct = len(true_entities &amp; pred_entities)
        nb_pred = len(pred_entities)
        nb_true = len(true_entities)

        p = nb_correct / nb_pred if nb_pred &gt; 0 else 0
        r = nb_correct / nb_true if nb_true &gt; 0 else 0
        f1 = 2 * p * r / (p + r) if p + r &gt; 0 else 0

        if output_dict:
            report_dict[type_name] = {
                &#34;precision&#34;: p,
                &#34;recall&#34;: r,
                &#34;f1-score&#34;: f1,
                &#34;support&#34;: nb_true,
            }
        else:
            report += row_fmt.format(
                *[type_name, p, r, f1, nb_true], width=width, digits=digits
            )

        ps.append(p)
        rs.append(r)
        f1s.append(f1)
        s.append(nb_true)

    if not output_dict:
        report += &#34;\n&#34;

    # compute averages
    nb_true = np.sum(s)

    for avg_type in avg_types:
        if avg_type == &#34;micro avg&#34;:
            # micro average
            p = precision_score(y_true, y_pred, suffix=suffix)
            r = recall_score(y_true, y_pred, suffix=suffix)
            f1 = f1_score(y_true, y_pred, suffix=suffix)
        elif avg_type == &#34;macro avg&#34;:
            # macro average
            p = np.average(ps)
            r = np.average(rs)
            f1 = np.average(f1s)
        elif avg_type == &#34;weighted avg&#34;:
            # weighted average
            p = np.average(ps, weights=s)
            r = np.average(rs, weights=s)
            f1 = np.average(f1s, weights=s)
        else:
            assert False, &#34;unexpected average: {}&#34;.format(avg_type)

        if output_dict:
            report_dict[avg_type] = {
                &#34;precision&#34;: p,
                &#34;recall&#34;: r,
                &#34;f1-score&#34;: f1,
                &#34;support&#34;: nb_true,
            }
        else:
            report += row_fmt.format(
                *[avg_type, p, r, f1, nb_true], width=width, digits=digits
            )

    if output_dict:
        return report_dict
    else:
        return report</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-functions">Functions</h2>
<dl>
<dt id="ktrain.text.ner.metrics.accuracy_score"><code class="name flex">
<span>def <span class="ident">accuracy_score</span></span>(<span>y_true, y_pred)</span>
</code></dt>
<dd>
<div class="desc"><p>Accuracy classification score.</p>
<p>In multilabel classification, this function computes subset accuracy:
the set of labels predicted for a sample must <em>exactly</em> match the
corresponding set of labels in y_true.</p>
<h2 id="args">Args</h2>
<p>y_true : 2d array. Ground truth (correct) target values.
y_pred : 2d array. Estimated targets as returned by a tagger.</p>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>score </code></dt>
<dd>float.</dd>
</dl>
<h2 id="example">Example</h2>
<pre><code class="language-python-repl">&gt;&gt;&gt; from seqeval.metrics import accuracy_score
&gt;&gt;&gt; y_true = [['O', 'O', 'O', 'B-MISC', 'I-MISC', 'I-MISC', 'O'], ['B-PER', 'I-PER', 'O']]
&gt;&gt;&gt; y_pred = [['O', 'O', 'B-MISC', 'I-MISC', 'I-MISC', 'I-MISC', 'O'], ['B-PER', 'I-PER', 'O']]
&gt;&gt;&gt; accuracy_score(y_true, y_pred)
0.80
</code></pre></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def accuracy_score(y_true, y_pred):
    &#34;&#34;&#34;Accuracy classification score.

    In multilabel classification, this function computes subset accuracy:
    the set of labels predicted for a sample must *exactly* match the
    corresponding set of labels in y_true.

    Args:
        y_true : 2d array. Ground truth (correct) target values.
        y_pred : 2d array. Estimated targets as returned by a tagger.

    Returns:
        score : float.

    Example:
        &gt;&gt;&gt; from seqeval.metrics import accuracy_score
        &gt;&gt;&gt; y_true = [[&#39;O&#39;, &#39;O&#39;, &#39;O&#39;, &#39;B-MISC&#39;, &#39;I-MISC&#39;, &#39;I-MISC&#39;, &#39;O&#39;], [&#39;B-PER&#39;, &#39;I-PER&#39;, &#39;O&#39;]]
        &gt;&gt;&gt; y_pred = [[&#39;O&#39;, &#39;O&#39;, &#39;B-MISC&#39;, &#39;I-MISC&#39;, &#39;I-MISC&#39;, &#39;I-MISC&#39;, &#39;O&#39;], [&#39;B-PER&#39;, &#39;I-PER&#39;, &#39;O&#39;]]
        &gt;&gt;&gt; accuracy_score(y_true, y_pred)
        0.80
    &#34;&#34;&#34;
    if any(isinstance(s, list) for s in y_true):
        y_true = [item for sublist in y_true for item in sublist]
        y_pred = [item for sublist in y_pred for item in sublist]

    nb_correct = sum(y_t == y_p for y_t, y_p in zip(y_true, y_pred))
    nb_true = len(y_true)

    score = nb_correct / nb_true

    return score</code></pre>
</details>
</dd>
<dt id="ktrain.text.ner.metrics.classification_report"><code class="name flex">
<span>def <span class="ident">classification_report</span></span>(<span>y_true, y_pred, digits=2, suffix=False, output_dict=False)</span>
</code></dt>
<dd>
<div class="desc"><p>Build a text report showing the main classification metrics.</p>
<h2 id="args">Args</h2>
<p>y_true : 2d array. Ground truth (correct) target values.
y_pred : 2d array. Estimated targets as returned by a classifier.
digits : int. Number of digits for formatting output floating point values.
output_dict : bool(default=False). If True, return output as dict else str.</p>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>report </code></dt>
<dd>string/dict. Summary of the precision, recall, F1 score for each class.</dd>
</dl>
<h2 id="examples">Examples</h2>
<blockquote>
<blockquote>
<blockquote>
<p>from seqeval.metrics import classification_report
y_true = [['O', 'O', 'O', 'B-MISC', 'I-MISC', 'I-MISC', 'O'], ['B-PER', 'I-PER', 'O']]
y_pred = [['O', 'O', 'B-MISC', 'I-MISC', 'I-MISC', 'I-MISC', 'O'], ['B-PER', 'I-PER', 'O']]
print(classification_report(y_true, y_pred))
precision
recall
f1-score
support
<BLANKLINE>
MISC
0.00
0.00
0.00
1
PER
1.00
1.00
1.00
1
<BLANKLINE>
micro avg
0.50
0.50
0.50
2
macro avg
0.50
0.50
0.50
2
weighted avg
0.50
0.50
0.50
2
<BLANKLINE></p>
</blockquote>
</blockquote>
</blockquote></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def classification_report(y_true, y_pred, digits=2, suffix=False, output_dict=False):
    &#34;&#34;&#34;Build a text report showing the main classification metrics.

    Args:
        y_true : 2d array. Ground truth (correct) target values.
        y_pred : 2d array. Estimated targets as returned by a classifier.
        digits : int. Number of digits for formatting output floating point values.
        output_dict : bool(default=False). If True, return output as dict else str.

    Returns:
        report : string/dict. Summary of the precision, recall, F1 score for each class.

    Examples:
        &gt;&gt;&gt; from seqeval.metrics import classification_report
        &gt;&gt;&gt; y_true = [[&#39;O&#39;, &#39;O&#39;, &#39;O&#39;, &#39;B-MISC&#39;, &#39;I-MISC&#39;, &#39;I-MISC&#39;, &#39;O&#39;], [&#39;B-PER&#39;, &#39;I-PER&#39;, &#39;O&#39;]]
        &gt;&gt;&gt; y_pred = [[&#39;O&#39;, &#39;O&#39;, &#39;B-MISC&#39;, &#39;I-MISC&#39;, &#39;I-MISC&#39;, &#39;I-MISC&#39;, &#39;O&#39;], [&#39;B-PER&#39;, &#39;I-PER&#39;, &#39;O&#39;]]
        &gt;&gt;&gt; print(classification_report(y_true, y_pred))
                     precision    recall  f1-score   support
        &lt;BLANKLINE&gt;
               MISC       0.00      0.00      0.00         1
                PER       1.00      1.00      1.00         1
        &lt;BLANKLINE&gt;
          micro avg       0.50      0.50      0.50         2
          macro avg       0.50      0.50      0.50         2
       weighted avg       0.50      0.50      0.50         2
        &lt;BLANKLINE&gt;
    &#34;&#34;&#34;
    true_entities = set(get_entities(y_true, suffix))
    pred_entities = set(get_entities(y_pred, suffix))

    name_width = 0
    d1 = defaultdict(set)
    d2 = defaultdict(set)
    for e in true_entities:
        d1[e[0]].add((e[1], e[2]))
        name_width = max(name_width, len(e[0]))
    for e in pred_entities:
        d2[e[0]].add((e[1], e[2]))

    avg_types = [&#34;micro avg&#34;, &#34;macro avg&#34;, &#34;weighted avg&#34;]

    if output_dict:
        report_dict = dict()
    else:
        avg_width = max([len(x) for x in avg_types])
        width = max(name_width, avg_width, digits)
        headers = [&#34;precision&#34;, &#34;recall&#34;, &#34;f1-score&#34;, &#34;support&#34;]
        head_fmt = &#34;{:&gt;{width}s} &#34; + &#34; {:&gt;9}&#34; * len(headers)
        report = head_fmt.format(&#34;&#34;, *headers, width=width)
        report += &#34;\n\n&#34;

        row_fmt = &#34;{:&gt;{width}s} &#34; + &#34; {:&gt;9.{digits}f}&#34; * 3 + &#34; {:&gt;9}\n&#34;

    ps, rs, f1s, s = [], [], [], []
    for type_name in sorted(d1.keys()):
        true_entities = d1[type_name]
        pred_entities = d2[type_name]
        nb_correct = len(true_entities &amp; pred_entities)
        nb_pred = len(pred_entities)
        nb_true = len(true_entities)

        p = nb_correct / nb_pred if nb_pred &gt; 0 else 0
        r = nb_correct / nb_true if nb_true &gt; 0 else 0
        f1 = 2 * p * r / (p + r) if p + r &gt; 0 else 0

        if output_dict:
            report_dict[type_name] = {
                &#34;precision&#34;: p,
                &#34;recall&#34;: r,
                &#34;f1-score&#34;: f1,
                &#34;support&#34;: nb_true,
            }
        else:
            report += row_fmt.format(
                *[type_name, p, r, f1, nb_true], width=width, digits=digits
            )

        ps.append(p)
        rs.append(r)
        f1s.append(f1)
        s.append(nb_true)

    if not output_dict:
        report += &#34;\n&#34;

    # compute averages
    nb_true = np.sum(s)

    for avg_type in avg_types:
        if avg_type == &#34;micro avg&#34;:
            # micro average
            p = precision_score(y_true, y_pred, suffix=suffix)
            r = recall_score(y_true, y_pred, suffix=suffix)
            f1 = f1_score(y_true, y_pred, suffix=suffix)
        elif avg_type == &#34;macro avg&#34;:
            # macro average
            p = np.average(ps)
            r = np.average(rs)
            f1 = np.average(f1s)
        elif avg_type == &#34;weighted avg&#34;:
            # weighted average
            p = np.average(ps, weights=s)
            r = np.average(rs, weights=s)
            f1 = np.average(f1s, weights=s)
        else:
            assert False, &#34;unexpected average: {}&#34;.format(avg_type)

        if output_dict:
            report_dict[avg_type] = {
                &#34;precision&#34;: p,
                &#34;recall&#34;: r,
                &#34;f1-score&#34;: f1,
                &#34;support&#34;: nb_true,
            }
        else:
            report += row_fmt.format(
                *[avg_type, p, r, f1, nb_true], width=width, digits=digits
            )

    if output_dict:
        return report_dict
    else:
        return report</code></pre>
</details>
</dd>
<dt id="ktrain.text.ner.metrics.end_of_chunk"><code class="name flex">
<span>def <span class="ident">end_of_chunk</span></span>(<span>prev_tag, tag, prev_type, type_)</span>
</code></dt>
<dd>
<div class="desc"><p>Checks if a chunk ended between the previous and current word.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>prev_tag</code></strong></dt>
<dd>previous chunk tag.</dd>
<dt><strong><code>tag</code></strong></dt>
<dd>current chunk tag.</dd>
<dt><strong><code>prev_type</code></strong></dt>
<dd>previous type.</dd>
<dt><strong><code>type_</code></strong></dt>
<dd>current type.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>chunk_end</code></dt>
<dd>boolean.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def end_of_chunk(prev_tag, tag, prev_type, type_):
    &#34;&#34;&#34;Checks if a chunk ended between the previous and current word.

    Args:
        prev_tag: previous chunk tag.
        tag: current chunk tag.
        prev_type: previous type.
        type_: current type.

    Returns:
        chunk_end: boolean.
    &#34;&#34;&#34;
    chunk_end = False

    if prev_tag == &#34;E&#34;:
        chunk_end = True
    if prev_tag == &#34;S&#34;:
        chunk_end = True

    if prev_tag == &#34;B&#34; and tag == &#34;B&#34;:
        chunk_end = True
    if prev_tag == &#34;B&#34; and tag == &#34;S&#34;:
        chunk_end = True
    if prev_tag == &#34;B&#34; and tag == &#34;O&#34;:
        chunk_end = True
    if prev_tag == &#34;I&#34; and tag == &#34;B&#34;:
        chunk_end = True
    if prev_tag == &#34;I&#34; and tag == &#34;S&#34;:
        chunk_end = True
    if prev_tag == &#34;I&#34; and tag == &#34;O&#34;:
        chunk_end = True

    if prev_tag != &#34;O&#34; and prev_tag != &#34;.&#34; and prev_type != type_:
        chunk_end = True

    return chunk_end</code></pre>
</details>
</dd>
<dt id="ktrain.text.ner.metrics.f1_score"><code class="name flex">
<span>def <span class="ident">f1_score</span></span>(<span>y_true, y_pred, average='micro', suffix=False)</span>
</code></dt>
<dd>
<div class="desc"><p>Compute the F1 score.</p>
<p>The F1 score can be interpreted as a weighted average of the precision and
recall, where an F1 score reaches its best value at 1 and worst score at 0.
The relative contribution of precision and recall to the F1 score are
equal. The formula for the F1 score is::</p>
<pre><code>F1 = 2 * (precision * recall) / (precision + recall)
</code></pre>
<h2 id="args">Args</h2>
<p>y_true : 2d array. Ground truth (correct) target values.
y_pred : 2d array. Estimated targets as returned by a tagger.</p>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>score </code></dt>
<dd>float.</dd>
</dl>
<h2 id="example">Example</h2>
<pre><code class="language-python-repl">&gt;&gt;&gt; from seqeval.metrics import f1_score
&gt;&gt;&gt; y_true = [['O', 'O', 'O', 'B-MISC', 'I-MISC', 'I-MISC', 'O'], ['B-PER', 'I-PER', 'O']]
&gt;&gt;&gt; y_pred = [['O', 'O', 'B-MISC', 'I-MISC', 'I-MISC', 'I-MISC', 'O'], ['B-PER', 'I-PER', 'O']]
&gt;&gt;&gt; f1_score(y_true, y_pred)
0.50
</code></pre></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def f1_score(y_true, y_pred, average=&#34;micro&#34;, suffix=False):
    &#34;&#34;&#34;Compute the F1 score.

    The F1 score can be interpreted as a weighted average of the precision and
    recall, where an F1 score reaches its best value at 1 and worst score at 0.
    The relative contribution of precision and recall to the F1 score are
    equal. The formula for the F1 score is::

        F1 = 2 * (precision * recall) / (precision + recall)

    Args:
        y_true : 2d array. Ground truth (correct) target values.
        y_pred : 2d array. Estimated targets as returned by a tagger.

    Returns:
        score : float.

    Example:
        &gt;&gt;&gt; from seqeval.metrics import f1_score
        &gt;&gt;&gt; y_true = [[&#39;O&#39;, &#39;O&#39;, &#39;O&#39;, &#39;B-MISC&#39;, &#39;I-MISC&#39;, &#39;I-MISC&#39;, &#39;O&#39;], [&#39;B-PER&#39;, &#39;I-PER&#39;, &#39;O&#39;]]
        &gt;&gt;&gt; y_pred = [[&#39;O&#39;, &#39;O&#39;, &#39;B-MISC&#39;, &#39;I-MISC&#39;, &#39;I-MISC&#39;, &#39;I-MISC&#39;, &#39;O&#39;], [&#39;B-PER&#39;, &#39;I-PER&#39;, &#39;O&#39;]]
        &gt;&gt;&gt; f1_score(y_true, y_pred)
        0.50
    &#34;&#34;&#34;
    true_entities = set(get_entities(y_true, suffix))
    pred_entities = set(get_entities(y_pred, suffix))

    nb_correct = len(true_entities &amp; pred_entities)
    nb_pred = len(pred_entities)
    nb_true = len(true_entities)

    p = nb_correct / nb_pred if nb_pred &gt; 0 else 0
    r = nb_correct / nb_true if nb_true &gt; 0 else 0
    score = 2 * p * r / (p + r) if p + r &gt; 0 else 0

    return score</code></pre>
</details>
</dd>
<dt id="ktrain.text.ner.metrics.get_entities"><code class="name flex">
<span>def <span class="ident">get_entities</span></span>(<span>seq, suffix=False)</span>
</code></dt>
<dd>
<div class="desc"><p>Gets entities from sequence.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>seq</code></strong> :&ensp;<code>list</code></dt>
<dd>sequence of labels.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>list</code></dt>
<dd>list of (chunk_type, chunk_start, chunk_end).</dd>
</dl>
<h2 id="example">Example</h2>
<pre><code class="language-python-repl">&gt;&gt;&gt; from seqeval.metrics.sequence_labeling import get_entities
&gt;&gt;&gt; seq = ['B-PER', 'I-PER', 'O', 'B-LOC']
&gt;&gt;&gt; get_entities(seq)
[('PER', 0, 1), ('LOC', 3, 3)]
</code></pre></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_entities(seq, suffix=False):
    &#34;&#34;&#34;Gets entities from sequence.

    Args:
        seq (list): sequence of labels.

    Returns:
        list: list of (chunk_type, chunk_start, chunk_end).

    Example:
        &gt;&gt;&gt; from seqeval.metrics.sequence_labeling import get_entities
        &gt;&gt;&gt; seq = [&#39;B-PER&#39;, &#39;I-PER&#39;, &#39;O&#39;, &#39;B-LOC&#39;]
        &gt;&gt;&gt; get_entities(seq)
        [(&#39;PER&#39;, 0, 1), (&#39;LOC&#39;, 3, 3)]
    &#34;&#34;&#34;

    def _validate_chunk(chunk, suffix):
        if chunk in [&#34;O&#34;, &#34;B&#34;, &#34;I&#34;, &#34;E&#34;, &#34;S&#34;]:
            return

        if suffix:
            if not (
                chunk.endswith(&#34;-B&#34;)
                or chunk.endswith(&#34;-I&#34;)
                or chunk.endswith(&#34;-E&#34;)
                or chunk.endswith(&#34;-S&#34;)
            ):
                warnings.warn(&#34;{} seems not to be NE tag.&#34;.format(chunk))

        else:
            if not (
                chunk.startswith(&#34;B-&#34;)
                or chunk.startswith(&#34;I-&#34;)
                or chunk.startswith(&#34;E-&#34;)
                or chunk.startswith(&#34;S-&#34;)
            ):
                warnings.warn(&#34;{} seems not to be NE tag.&#34;.format(chunk))

    # for nested list
    if any(isinstance(s, list) for s in seq):
        seq = [item for sublist in seq for item in sublist + [&#34;O&#34;]]

    prev_tag = &#34;O&#34;
    prev_type = &#34;&#34;
    begin_offset = 0
    chunks = []
    for i, chunk in enumerate(seq + [&#34;O&#34;]):
        _validate_chunk(chunk, suffix)

        if suffix:
            tag = chunk[-1]
            type_ = chunk[:-1].rsplit(&#34;-&#34;, maxsplit=1)[0] or &#34;_&#34;
        else:
            tag = chunk[0]
            type_ = chunk[1:].split(&#34;-&#34;, maxsplit=1)[-1] or &#34;_&#34;

        if end_of_chunk(prev_tag, tag, prev_type, type_):
            chunks.append((prev_type, begin_offset, i - 1))
        if start_of_chunk(prev_tag, tag, prev_type, type_):
            begin_offset = i
        prev_tag = tag
        prev_type = type_

    return chunks</code></pre>
</details>
</dd>
<dt id="ktrain.text.ner.metrics.performance_measure"><code class="name flex">
<span>def <span class="ident">performance_measure</span></span>(<span>y_true, y_pred)</span>
</code></dt>
<dd>
<div class="desc"><p>Compute the performance metrics: TP, FP, FN, TN</p>
<h2 id="args">Args</h2>
<p>y_true : 2d array. Ground truth (correct) target values.
y_pred : 2d array. Estimated targets as returned by a tagger.</p>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>performance_dict </code></dt>
<dd>dict</dd>
</dl>
<h2 id="example">Example</h2>
<pre><code class="language-python-repl">&gt;&gt;&gt; from seqeval.metrics import performance_measure
&gt;&gt;&gt; y_true = [['O', 'O', 'O', 'B-MISC', 'I-MISC', 'O', 'B-ORG'], ['B-PER', 'I-PER', 'O', 'B-PER']]
&gt;&gt;&gt; y_pred = [['O', 'O', 'B-MISC', 'I-MISC', 'I-MISC', 'O', 'O'], ['B-PER', 'I-PER', 'O', 'B-MISC']]
&gt;&gt;&gt; performance_measure(y_true, y_pred)
{'TP': 3, 'FP': 3, 'FN': 1, 'TN': 4}
</code></pre></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def performance_measure(y_true, y_pred):
    &#34;&#34;&#34;
    Compute the performance metrics: TP, FP, FN, TN

    Args:
        y_true : 2d array. Ground truth (correct) target values.
        y_pred : 2d array. Estimated targets as returned by a tagger.

    Returns:
        performance_dict : dict

    Example:
        &gt;&gt;&gt; from seqeval.metrics import performance_measure
        &gt;&gt;&gt; y_true = [[&#39;O&#39;, &#39;O&#39;, &#39;O&#39;, &#39;B-MISC&#39;, &#39;I-MISC&#39;, &#39;O&#39;, &#39;B-ORG&#39;], [&#39;B-PER&#39;, &#39;I-PER&#39;, &#39;O&#39;, &#39;B-PER&#39;]]
        &gt;&gt;&gt; y_pred = [[&#39;O&#39;, &#39;O&#39;, &#39;B-MISC&#39;, &#39;I-MISC&#39;, &#39;I-MISC&#39;, &#39;O&#39;, &#39;O&#39;], [&#39;B-PER&#39;, &#39;I-PER&#39;, &#39;O&#39;, &#39;B-MISC&#39;]]
        &gt;&gt;&gt; performance_measure(y_true, y_pred)
        {&#39;TP&#39;: 3, &#39;FP&#39;: 3, &#39;FN&#39;: 1, &#39;TN&#39;: 4}
    &#34;&#34;&#34;
    performance_dict = dict()
    if any(isinstance(s, list) for s in y_true):
        y_true = [item for sublist in y_true for item in sublist]
        y_pred = [item for sublist in y_pred for item in sublist]
    performance_dict[&#34;TP&#34;] = sum(
        y_t == y_p for y_t, y_p in zip(y_true, y_pred) if ((y_t != &#34;O&#34;) or (y_p != &#34;O&#34;))
    )
    performance_dict[&#34;FP&#34;] = sum(
        ((y_t != y_p) and (y_p != &#34;O&#34;)) for y_t, y_p in zip(y_true, y_pred)
    )
    performance_dict[&#34;FN&#34;] = sum(
        ((y_t != &#34;O&#34;) and (y_p == &#34;O&#34;)) for y_t, y_p in zip(y_true, y_pred)
    )
    performance_dict[&#34;TN&#34;] = sum(
        (y_t == y_p == &#34;O&#34;) for y_t, y_p in zip(y_true, y_pred)
    )

    return performance_dict</code></pre>
</details>
</dd>
<dt id="ktrain.text.ner.metrics.precision_score"><code class="name flex">
<span>def <span class="ident">precision_score</span></span>(<span>y_true, y_pred, average='micro', suffix=False)</span>
</code></dt>
<dd>
<div class="desc"><p>Compute the precision.</p>
<p>The precision is the ratio <code>tp / (tp + fp)</code> where <code>tp</code> is the number of
true positives and <code>fp</code> the number of false positives. The precision is
intuitively the ability of the classifier not to label as positive a sample.</p>
<p>The best value is 1 and the worst value is 0.</p>
<h2 id="args">Args</h2>
<p>y_true : 2d array. Ground truth (correct) target values.
y_pred : 2d array. Estimated targets as returned by a tagger.</p>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>score </code></dt>
<dd>float.</dd>
</dl>
<h2 id="example">Example</h2>
<pre><code class="language-python-repl">&gt;&gt;&gt; from seqeval.metrics import precision_score
&gt;&gt;&gt; y_true = [['O', 'O', 'O', 'B-MISC', 'I-MISC', 'I-MISC', 'O'], ['B-PER', 'I-PER', 'O']]
&gt;&gt;&gt; y_pred = [['O', 'O', 'B-MISC', 'I-MISC', 'I-MISC', 'I-MISC', 'O'], ['B-PER', 'I-PER', 'O']]
&gt;&gt;&gt; precision_score(y_true, y_pred)
0.50
</code></pre></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def precision_score(y_true, y_pred, average=&#34;micro&#34;, suffix=False):
    &#34;&#34;&#34;Compute the precision.

    The precision is the ratio ``tp / (tp + fp)`` where ``tp`` is the number of
    true positives and ``fp`` the number of false positives. The precision is
    intuitively the ability of the classifier not to label as positive a sample.

    The best value is 1 and the worst value is 0.

    Args:
        y_true : 2d array. Ground truth (correct) target values.
        y_pred : 2d array. Estimated targets as returned by a tagger.

    Returns:
        score : float.

    Example:
        &gt;&gt;&gt; from seqeval.metrics import precision_score
        &gt;&gt;&gt; y_true = [[&#39;O&#39;, &#39;O&#39;, &#39;O&#39;, &#39;B-MISC&#39;, &#39;I-MISC&#39;, &#39;I-MISC&#39;, &#39;O&#39;], [&#39;B-PER&#39;, &#39;I-PER&#39;, &#39;O&#39;]]
        &gt;&gt;&gt; y_pred = [[&#39;O&#39;, &#39;O&#39;, &#39;B-MISC&#39;, &#39;I-MISC&#39;, &#39;I-MISC&#39;, &#39;I-MISC&#39;, &#39;O&#39;], [&#39;B-PER&#39;, &#39;I-PER&#39;, &#39;O&#39;]]
        &gt;&gt;&gt; precision_score(y_true, y_pred)
        0.50
    &#34;&#34;&#34;
    true_entities = set(get_entities(y_true, suffix))
    pred_entities = set(get_entities(y_pred, suffix))

    nb_correct = len(true_entities &amp; pred_entities)
    nb_pred = len(pred_entities)

    score = nb_correct / nb_pred if nb_pred &gt; 0 else 0

    return score</code></pre>
</details>
</dd>
<dt id="ktrain.text.ner.metrics.recall_score"><code class="name flex">
<span>def <span class="ident">recall_score</span></span>(<span>y_true, y_pred, average='micro', suffix=False)</span>
</code></dt>
<dd>
<div class="desc"><p>Compute the recall.</p>
<p>The recall is the ratio <code>tp / (tp + fn)</code> where <code>tp</code> is the number of
true positives and <code>fn</code> the number of false negatives. The recall is
intuitively the ability of the classifier to find all the positive samples.</p>
<p>The best value is 1 and the worst value is 0.</p>
<h2 id="args">Args</h2>
<p>y_true : 2d array. Ground truth (correct) target values.
y_pred : 2d array. Estimated targets as returned by a tagger.</p>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>score </code></dt>
<dd>float.</dd>
</dl>
<h2 id="example">Example</h2>
<pre><code class="language-python-repl">&gt;&gt;&gt; from seqeval.metrics import recall_score
&gt;&gt;&gt; y_true = [['O', 'O', 'O', 'B-MISC', 'I-MISC', 'I-MISC', 'O'], ['B-PER', 'I-PER', 'O']]
&gt;&gt;&gt; y_pred = [['O', 'O', 'B-MISC', 'I-MISC', 'I-MISC', 'I-MISC', 'O'], ['B-PER', 'I-PER', 'O']]
&gt;&gt;&gt; recall_score(y_true, y_pred)
0.50
</code></pre></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def recall_score(y_true, y_pred, average=&#34;micro&#34;, suffix=False):
    &#34;&#34;&#34;Compute the recall.

    The recall is the ratio ``tp / (tp + fn)`` where ``tp`` is the number of
    true positives and ``fn`` the number of false negatives. The recall is
    intuitively the ability of the classifier to find all the positive samples.

    The best value is 1 and the worst value is 0.

    Args:
        y_true : 2d array. Ground truth (correct) target values.
        y_pred : 2d array. Estimated targets as returned by a tagger.

    Returns:
        score : float.

    Example:
        &gt;&gt;&gt; from seqeval.metrics import recall_score
        &gt;&gt;&gt; y_true = [[&#39;O&#39;, &#39;O&#39;, &#39;O&#39;, &#39;B-MISC&#39;, &#39;I-MISC&#39;, &#39;I-MISC&#39;, &#39;O&#39;], [&#39;B-PER&#39;, &#39;I-PER&#39;, &#39;O&#39;]]
        &gt;&gt;&gt; y_pred = [[&#39;O&#39;, &#39;O&#39;, &#39;B-MISC&#39;, &#39;I-MISC&#39;, &#39;I-MISC&#39;, &#39;I-MISC&#39;, &#39;O&#39;], [&#39;B-PER&#39;, &#39;I-PER&#39;, &#39;O&#39;]]
        &gt;&gt;&gt; recall_score(y_true, y_pred)
        0.50
    &#34;&#34;&#34;
    true_entities = set(get_entities(y_true, suffix))
    pred_entities = set(get_entities(y_pred, suffix))

    nb_correct = len(true_entities &amp; pred_entities)
    nb_true = len(true_entities)

    score = nb_correct / nb_true if nb_true &gt; 0 else 0

    return score</code></pre>
</details>
</dd>
<dt id="ktrain.text.ner.metrics.start_of_chunk"><code class="name flex">
<span>def <span class="ident">start_of_chunk</span></span>(<span>prev_tag, tag, prev_type, type_)</span>
</code></dt>
<dd>
<div class="desc"><p>Checks if a chunk started between the previous and current word.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>prev_tag</code></strong></dt>
<dd>previous chunk tag.</dd>
<dt><strong><code>tag</code></strong></dt>
<dd>current chunk tag.</dd>
<dt><strong><code>prev_type</code></strong></dt>
<dd>previous type.</dd>
<dt><strong><code>type_</code></strong></dt>
<dd>current type.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>chunk_start</code></dt>
<dd>boolean.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def start_of_chunk(prev_tag, tag, prev_type, type_):
    &#34;&#34;&#34;Checks if a chunk started between the previous and current word.

    Args:
        prev_tag: previous chunk tag.
        tag: current chunk tag.
        prev_type: previous type.
        type_: current type.

    Returns:
        chunk_start: boolean.
    &#34;&#34;&#34;
    chunk_start = False

    if tag == &#34;B&#34;:
        chunk_start = True
    if tag == &#34;S&#34;:
        chunk_start = True

    if prev_tag == &#34;E&#34; and tag == &#34;E&#34;:
        chunk_start = True
    if prev_tag == &#34;E&#34; and tag == &#34;I&#34;:
        chunk_start = True
    if prev_tag == &#34;S&#34; and tag == &#34;E&#34;:
        chunk_start = True
    if prev_tag == &#34;S&#34; and tag == &#34;I&#34;:
        chunk_start = True
    if prev_tag == &#34;O&#34; and tag == &#34;E&#34;:
        chunk_start = True
    if prev_tag == &#34;O&#34; and tag == &#34;I&#34;:
        chunk_start = True

    if tag != &#34;O&#34; and tag != &#34;.&#34; and prev_type != type_:
        chunk_start = True

    return chunk_start</code></pre>
</details>
</dd>
</dl>
</section>
<section>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="ktrain.text.ner" href="index.html">ktrain.text.ner</a></code></li>
</ul>
</li>
<li><h3><a href="#header-functions">Functions</a></h3>
<ul class="">
<li><code><a title="ktrain.text.ner.metrics.accuracy_score" href="#ktrain.text.ner.metrics.accuracy_score">accuracy_score</a></code></li>
<li><code><a title="ktrain.text.ner.metrics.classification_report" href="#ktrain.text.ner.metrics.classification_report">classification_report</a></code></li>
<li><code><a title="ktrain.text.ner.metrics.end_of_chunk" href="#ktrain.text.ner.metrics.end_of_chunk">end_of_chunk</a></code></li>
<li><code><a title="ktrain.text.ner.metrics.f1_score" href="#ktrain.text.ner.metrics.f1_score">f1_score</a></code></li>
<li><code><a title="ktrain.text.ner.metrics.get_entities" href="#ktrain.text.ner.metrics.get_entities">get_entities</a></code></li>
<li><code><a title="ktrain.text.ner.metrics.performance_measure" href="#ktrain.text.ner.metrics.performance_measure">performance_measure</a></code></li>
<li><code><a title="ktrain.text.ner.metrics.precision_score" href="#ktrain.text.ner.metrics.precision_score">precision_score</a></code></li>
<li><code><a title="ktrain.text.ner.metrics.recall_score" href="#ktrain.text.ner.metrics.recall_score">recall_score</a></code></li>
<li><code><a title="ktrain.text.ner.metrics.start_of_chunk" href="#ktrain.text.ner.metrics.start_of_chunk">start_of_chunk</a></code></li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc" title="pdoc: Python API documentation generator"><cite>pdoc</cite> 0.10.0</a>.</p>
</footer>
</body>
</html>